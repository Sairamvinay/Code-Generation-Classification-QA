%
% File acl2019.tex
%
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2019}
\usepackage{times}
\usepackage{latexsym}

\usepackage{url}

\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{Text Classification for Source Code Related Questions}

\author{
  Jason Song \\ \And
  David Tomassi \\ \And
  Siddhartha Punj\\ \AND
  Jailan Sabet\\ \And
  Sairamvinay Vijayaraghavan}

\date{}

\begin{document}
\maketitle

\section{Introduction}
Classification of text is an important field of research and a core
task in Natural Language Processing (NLP). It spans many different
domains from determining “fake” news, finding spam emails, and
language detection. A common problem in software development is
finding the appropriate code snippet for a task. There are communities
for software development that allow people to ask questions and
provide answers such as Stack Overflow. The issue that arises is that
many people will provide completely different answers for a particular
question in which many cases the answers are wrong. We pose the task
of determining if a pair of a question/problem and  a corresponding
code snippet is appropriate or not as a binary classification problem.

Our goal is to learn from these pairs the correct code snippets for
certain problems. An example of this is the code snippet,
"shutil.copy('file.txt', 'file2.txt')" and the question, “Is this how
to copy one file's contents to another in python?”. We will make use
of deep learning models to learn from a code corpus and corresponding
intent converted into ‘yes’ or ‘no’ labelled data. The creation of the
‘yes/no’ questions will be conducted by appending “Is this how” and
similar wordings to make the intent into a question. We will have the
positive “yes” questions as the standard answer to the question. The
negative “no” questions will be created by sampling different code
snippets from other pairs and creating new pairs with these new code
snippets with the original question. So, we plan to ensure that an
equal number of positive and negative samples are used for training
our models for classification.

\section{Dataset}
For validating our hypothesis, we plan to choose working on a
Code/Natural Language Challenge (CoNaLa) dataset. This dataset
contains 598k json lines formatted files automatically data mined from
StackOverflow which contains the question ID, intent of the code
snippet presented, code snippet, parent answer post ID, and an unique
ID for the post. In addition to the above set, there is another 2K
manual curated data which contains question ID, intent, rewritten
intent and also the code snippet. So, we plan to use the combination
of both these data sets. These datasets would provide all the snippets
in Python3 Programming Language.

\section{Baseline Model}
Since we use the CoNALA dataset, the CoNaLa will provide the intent
which is the input question sequence and the output code snippet as
the context information. Then, we will process the input question by
prepending some interrogative words (such as the common phrase “is
this how this process...”) in front of the input sequence which will
convert it into the question format. For the code, we won’t be doing a
lot of work but we will break it up into tokens which are
understandable by the machine (code can be broken up into tokens for
learning the structure of the code done for a task).

We plan to alter these datasets in order to suit to our task by
performing some additions to the intent by rephrasing it as a question
while we utilize the appropriate snippet as a context to formulate the
problem to be solved and then add labels as “yes” or “no” for training
our models.

We plan to also use further analysis for developing the context
related vocabulary by using a code related corpus which provides the
models to train by learning the vocabulary for analyzing the tokens
present in the Python3 Programming language. So for this task, we
intend to utilize a large Python3 vocabulary corpus which contains the
key tokens present in the programming language. For this, we intend to
use some different corpuses: StaQC (stack overflow mined dataset QA
pairs but there is an extracted vocabulary file which we use); Code
Docstring corpus for extracting the vocabulary tokens useful for
learning the context related vocabulary. We are still in pursuit of
larger corpus for training our models but we will implement these
corpuses as a base for building our contextual vocabulary for now.

Our problem setting is a sequence-to-sequence with attention model
where the input is a question sequence and the output is a code
sequence. We will map each question into a real vector domain, a
popular technique when working with natural language text called word
embedding. We plan to use some pre-trained word embeddings such as
GloVe or Word2Vec where words are encoded as real-valued vectors in a
high dimensional space, where the similarity/global contextual meaning
between words translates to closeness in the vector space. Then, we
can feed the hidden state vector from the seq2seq output to the
forward neural network model.  Forward neural network model will learn
the labels for the code snippet’s correctness since we will update the
loss of the model each time and incorporate the seq2seq loss together
in order to increase the learning performance. Then,using a dense
output layer with a single neuron, sigmoid activation function can
help to make 0 or 1 predictions for the two classes (correct and
incorrect) in the problem. 

\section{Evaluation}
In order to measure how effectively our model is able to classify
CoNaLa rephrased question/snippet pairs accurately, we can use F1
scores, which utilizes both precision and recall. Precision is the
fraction of relevant instances among total retrieved instances (True
positives/True positives + False Positives), and recall is the
fraction of the total amount of relevant instances that were actually
retrieved (True positives/True positives + False negatives). In
addition, we can also use accuracy- equation: (True positives + True
negatives)/N, where N is the total number of samples in order to
calculate how accurately our model classifies a rephrased question
with its correct code snippet.

We have three proposed methods to create a test set for the
evaluation. The first method involves using the original dataset,
CoNaLa, by assigning only 80\% of the modified dataset to training our
model and utilizing the remaining 20\% as a test set. This method
would ensure that the training and test sets are consistent, but will
contain the least amount of examples per set. For the second method,
we can adapt other corpora, such as StaQC or Code Docstring, to
construct a dataset separate from the training set; the modified
dataset from CoNaLa, in this case, would all be used for training.
This method would allow for larger datasets, but less consistency
among the training and test sets. Finally, for the last method, we can
generate a completely new dataset by web scraping relevant websites,
such as Stack Overflow (similar to the method used to create CoNaLa),
adapting the raw collected data into a test set, leaving the various
other corpora (CoNaLa, StaQC, and Code Docstring) available for
training our model. This method would result in the large amount and
variety of examples for the training and test set, but would be the
least consistent and most time consuming.

As a separate method of evaluating the model, we also plan to ask
classmates to provide inputs (a snippet of code and a question about
the code) for our classifier and report the overall accuracy of the
total resulting outputs as well as the largest number of times our
model consecutively determines the correct answer.

In order to increase the performance of our training model, we can use
the AdamW optimizer. The AdamW optimizer combines the effectiveness of
AdaGrad, which improves performance on problems with sparse gradients,
as well as RMSProp, which maintains parameter learning rates that are
adapted based on how quickly the weights of the model are changing,
which means it does well on noisy data (nonstandard words, missing
punctuation, abbreviations, etc.). To summarize, it can modify the
weight decay and learning rate of a model separately, which results in
vastly improved general performance. Since we are working with a
standard classification problem, we can use binary cross entropy as
the loss function in order to determine how well the model classifies
the data. 

Finally, in order to reduce time for training, we can potentially
implement early stopping in the model so that we break from the
training loop early if the current validation set loss happens to
exceed the previous loss for determining whether a code snippet
matches the rephrased task question or not.


\end{document}
